{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n main\n",
    "records = parse_tsv(LEADERBOARD_LINES)\n",
    "leaderboard = list(parse_leaderboard(records))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -n main\n",
    "# task_tests = {\n",
    "#     _: list(load_task(_, PUBLIC, TEST))\n",
    "#     for _ in TASKS\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %run -n main\n",
    "# for conf in GRID_CONFS:\n",
    "#     for task in TASKS:\n",
    "#         exp_dir = find_grid_exp_dir(EXPS_DIR, conf.id, conf.model, task)\n",
    "#         if not exp_dir:\n",
    "#             log(f'missing {conf.id} {conf.model} {task}')\n",
    "#             continue\n",
    "\n",
    "#         if not grid_exp_finised(exp_dir, task):\n",
    "#             log(f'unfinished {conf.id} {conf.model} {task}')\n",
    "#             continue\n",
    "            \n",
    "#         if grid_preds_exist(conf.id, task):\n",
    "#             continue\n",
    "\n",
    "#         log('id: %r, task: %r', conf.id, task)\n",
    "#         preds = infer_jiant(exp_dir, task, task_tests[task], batch_size=64)\n",
    "#         dump_grid_preds(conf.id, task, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -n main\n",
    "# task_targets = {\n",
    "#     _: list(load_task(_, PRIVATE, TEST))\n",
    "#     for _ in TASKS\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %run -n main\n",
    "# conf_task_metrics = []\n",
    "# for conf in log_progress(GRID_CONFS):\n",
    "#     for task in TASKS:\n",
    "#         if grid_preds_exist(conf.id, task):\n",
    "#             preds = list(load_grid_preds(conf.id, task))\n",
    "#             metrics = eval(task, preds, task_targets[task])\n",
    "#             conf_task_metrics.append([conf.id, task, metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -n main\n",
    "# lines = format_jl(conf_task_metrics)\n",
    "# dump_lines(lines, CONF_TASK_METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n main.py\n",
    "lines = load_lines(CONF_TASK_METRICS)\n",
    "conf_task_metrics = list(parse_jl(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_task_scores = [\n",
    "    (id, task, select_score(task, metrics))\n",
    "    for id, task, metrics in conf_task_metrics\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n main\n",
    "task_train_sizes = {\n",
    "    _: sum(1 for _ in load_task(_, PUBLIC, TRAIN))\n",
    "    for _ in TASKS\n",
    "    if _ != LIDIRUS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n main\n",
    "show_grid_scores(leaderboard, conf_task_scores, task_train_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n main\n",
    "tasks = TASKS\n",
    "# tasks = [DANETQA, MUSERC, RUCOS, RUSSE]\n",
    "# show_seed_scores(leaderboard, conf_task_scores, tasks=tasks)\n",
    "show_seed_scores2(conf_task_scores, tasks=tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -n main\n",
    "# for conf in GRID_CONFS:\n",
    "#     print(f'python main.py s3 sync ~/exps/{conf.id}/{conf.model} //exps/{conf.id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download top exps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -n main\n",
    "# for model in MODELS:\n",
    "#     for task in TASKS:\n",
    "#         if task == LIDIRUS:\n",
    "#             continue\n",
    "\n",
    "#         id = select_top_conf(task, model, conf_task_scores)\n",
    "#         if not id:\n",
    "#             log(f'missing {model} {task}')\n",
    "#             continue\n",
    "            \n",
    "#         leaderboard_score = score_value(find_leaderboard_score(model, task, leaderboard))\n",
    "#         grid_score = score_value(find_grid_score(id, task, conf_task_scores))\n",
    "#         label = '!' if grid_score < leaderboard_score else ' '\n",
    "\n",
    "#         print(f'{label} {leaderboard_score:0.3f} {grid_score:0.3f} {id} {task:<10} {model}')\n",
    "#         print(f'python main.py s3 sync //exps/{id}/{task} exps/{model}/{task}')\n",
    "#     print(f'python main.py s3 sync //exps/{id}/transformers_cache exps/{model}/transformers_cache')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected = []\n",
    "# for model in MODELS:\n",
    "#     for task in TASKS:\n",
    "#         name = TERRA if task == LIDIRUS else task\n",
    "#         dir = f'exps/{model}/{name}'\n",
    "#         timestamp = path_modified(dir)\n",
    "#         if timestamp.day == 29:\n",
    "#             selected.append((model, task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -n main\n",
    "# for model in MODELS:\n",
    "#     for task in TASKS:\n",
    "#         if (model, task) in selected:\n",
    "#             print(f'python main.py docker build exps/{model} {task} {model}-{task}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model in MODELS:\n",
    "#     for task in TASKS:\n",
    "#         print(f'python main.py docker push {model}-{task}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model in MODELS:\n",
    "#     for task in TASKS:\n",
    "#         input = task_path(task, PUBLIC, TEST, '~/data')\n",
    "#         output = f'~/preds/docker/{model}/{task}.jl'\n",
    "#         if not exists(expanduser(output)):\n",
    "#             print(f'docker run --gpus all --interactive --rm {model}-{task} --batch-size=32 < {input} > {output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_scores = []\n",
    "for model in log_progress(MODELS):\n",
    "    for task in TASKS:\n",
    "        path = expanduser(f'~/preds/docker/{model}/{task}.jl')\n",
    "        if not exists(path):\n",
    "            continue\n",
    "\n",
    "        preds = list(load_jl(path))\n",
    "\n",
    "        path = task_path(task, PRIVATE, TEST)\n",
    "        targets = list(load_jl(path))\n",
    "\n",
    "        metrics = eval(task, preds, targets)\n",
    "        score = select_score(task, metrics)\n",
    "        docker_scores.append([model, task, score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run -n main\n",
    "table = show_docker_leaderboard(leaderboard, docker_scores)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model in MODELS:\n",
    "#     for task in TASKS:\n",
    "#         for index in [1, 2, 3, 4, 5]:\n",
    "#             input_size = 2000\n",
    "#             batch_size = 32\n",
    "# #             input_size = 1\n",
    "# #             batch_size = 1\n",
    "#             print(\n",
    "#                 f'python main.py docker bench {model}-{task} ~/data {task} --input-size={input_size} --batch-size={batch_size} '\n",
    "#                 f'> ~/benches/{model}/{task}/{input_size}_{batch_size}_{index:02d}.jl'\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n main\n",
    "bench_registry = list(list_bench_registry())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n main\n",
    "records = query_bench_registry(\n",
    "    bench_registry,\n",
    "    input_size=1,\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "records = query_bench_registry(\n",
    "    bench_registry,\n",
    "    input_size=2000,\n",
    "    batch_size=32,\n",
    "    task=(TERRA),\n",
    "    model=(RUBERT, RUGPT3_MEDIUM)\n",
    ")\n",
    "\n",
    "benches = [\n",
    "    load_bench_registry(_)\n",
    "    for _ in records\n",
    "]\n",
    "\n",
    "# task=(DANETQA, PARUS, RCB, RWSD, RUSSE, TERRA, LIDIRUS)\n",
    "# task=(RUCOS, MUSERC)\n",
    "\n",
    "# model=(RUBERT, RUBERT_CONVERSATIONAL, BERT_MULTILINGUAL)\n",
    "# model=(RUGPT3_LARGE, RUGPT3_MEDIUM, RUGPT3_SMALL)\n",
    "\n",
    "show_benches(benches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n main\n",
    "bench_groups = list(load_group_benches(bench_registry))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpu ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n main\n",
    "show_gpu_ram_bench_report(bench_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n main\n",
    "table = show_gpu_ram_bench_report2(bench_groups)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n main\n",
    "show_gpu_ram_hub_size_bench_report(bench_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n main\n",
    "show_init_time_bench_report(bench_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n main\n",
    "show_proc_time_bench_report(bench_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -n main\n",
    "table = show_rps_bench_report(bench_groups)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%run -n main\n",
    "show_rps_order_bench_report(bench_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# score perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %run -n main\n",
    "# show_score_perf(docker_scores, leaderboard, bench_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "0.23.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
