{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# leaderboard"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%run -n main\n",
        "records = parse_tsv(LEADERBOARD_LINES)\n",
        "leaderboard = list(parse_leaderboard(records))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# grid"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## preds"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# %run -n main\n",
        "# task_tests = {\n",
        "#     _: list(load_task(_, PUBLIC, TEST))\n",
        "#     for _ in TASKS\n",
        "# }"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# %run -n main\n",
        "# for conf in GRID_CONFS:\n",
        "#     for task in TASKS:\n",
        "#         exp_dir = find_grid_exp_dir(EXPS_DIR, conf.id, conf.model, task)\n",
        "#         if not exp_dir:\n",
        "#             log(f'missing {conf.id} {conf.model} {task}')\n",
        "#             continue\n",
        "\n",
        "#         if not grid_exp_finised(exp_dir, task):\n",
        "#             log(f'unfinished {conf.id} {conf.model} {task}')\n",
        "#             continue\n",
        "            \n",
        "#         if grid_preds_exist(conf.id, task):\n",
        "#             continue\n",
        "\n",
        "#         log('id: %r, task: %r', conf.id, task)\n",
        "#         preds = infer_jiant(exp_dir, task, task_tests[task], batch_size=64)\n",
        "#         dump_grid_preds(conf.id, task, preds)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## eval"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%run -n main\n",
        "task_targets = {\n",
        "    _: list(load_task(_, PRIVATE, TEST))\n",
        "    for _ in TASKS\n",
        "}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%run -n main\n",
        "conf_task_scores = []\n",
        "for conf in log_progress(GRID_CONFS):\n",
        "    for task in TASKS:\n",
        "        if grid_preds_exist(conf.id, task):\n",
        "            preds = list(load_grid_preds(conf.id, task))\n",
        "            metrics = eval(task, preds, task_targets[task])\n",
        "            score = select_score(task, metrics)\n",
        "            conf_task_scores.append([conf.id, task, score])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## show"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%run -n main\n",
        "task_train_sizes = {\n",
        "    _: sum(1 for _ in load_task(_, PUBLIC, TRAIN))\n",
        "    for _ in TASKS\n",
        "    if _ != LIDIRUS\n",
        "}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%run -n main\n",
        "show_grid_scores(leaderboard, conf_task_scores, task_train_sizes)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## upload"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# %run -n main\n",
        "# for conf in GRID_CONFS:\n",
        "#     print(f'python main.py s3 sync ~/exps/{conf.id}/{conf.model} //exps/{conf.id}')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# docker"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## download top exps"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# %run -n main\n",
        "# for model in MODELS:\n",
        "#     for task in TASKS:\n",
        "#         if task == LIDIRUS:\n",
        "#             continue\n",
        "\n",
        "#         id = select_top_conf(task, model, conf_task_scores)\n",
        "#         if not id:\n",
        "#             log(f'missing {model} {task}')\n",
        "#             continue\n",
        "            \n",
        "#         leaderboard_score = score_value(find_leaderboard_score(model, task, leaderboard))\n",
        "#         grid_score = score_value(find_grid_score(id, task, conf_task_scores))\n",
        "#         label = '!' if grid_score < leaderboard_score else ' '\n",
        "\n",
        "#         print(f'{label} {leaderboard_score:0.3f} {grid_score:0.3f} {id} {task:<10} {model}')\n",
        "#         print(f'python main.py s3 sync //exps/{id}/{task} exps/{model}/{task}')\n",
        "#     print(f'python main.py s3 sync //exps/{id}/transformers_cache exps/{model}/transformers_cache')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## build"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# selected = []\n",
        "# for model in MODELS:\n",
        "#     for task in TASKS:\n",
        "#         name = TERRA if task == LIDIRUS else task\n",
        "#         dir = f'exps/{model}/{name}'\n",
        "#         timestamp = path_modified(dir)\n",
        "#         if timestamp.day == 29:\n",
        "#             selected.append((model, task))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# %run -n main\n",
        "# for model in MODELS:\n",
        "#     for task in TASKS:\n",
        "#         if (model, task) in selected:\n",
        "#             print(f'python main.py docker build exps/{model} {task} {model}-{task}')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## upload"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# for model in MODELS:\n",
        "#     for task in TASKS:\n",
        "#         print(f'python main.py docker push {model}-{task}')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## preds"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# for model in MODELS:\n",
        "#     for task in TASKS:\n",
        "#         input = task_path(task, PUBLIC, TEST, '~/data')\n",
        "#         output = f'~/preds/docker/{model}/{task}.jl'\n",
        "#         if not exists(expanduser(output)):\n",
        "#             print(f'docker run --gpus all --interactive --rm {model}-{task} --batch-size=32 < {input} > {output}')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "docker_scores = []\n",
        "for model in log_progress(MODELS):\n",
        "    for task in TASKS:\n",
        "        path = expanduser(f'~/preds/docker/{model}/{task}.jl')\n",
        "        if not exists(path):\n",
        "            continue\n",
        "\n",
        "        preds = list(load_jl(path))\n",
        "\n",
        "        path = task_path(task, PRIVATE, TEST)\n",
        "        targets = list(load_jl(path))\n",
        "\n",
        "        metrics = eval(task, preds, targets)\n",
        "        score = select_score(task, metrics)\n",
        "        docker_scores.append([model, task, score])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%run -n main\n",
        "table = show_docker_leaderboard(leaderboard, docker_scores)\n",
        "table"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# bench"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## registry"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# for model in MODELS:\n",
        "#     for task in TASKS:\n",
        "#         for index in [1, 2, 3, 4, 5]:\n",
        "#             input_size = 2000\n",
        "#             batch_size = 32\n",
        "# #             input_size = 1\n",
        "# #             batch_size = 1\n",
        "#             print(\n",
        "#                 f'python main.py bench {model}-{task} ~/data {task} --input-size={input_size} --batch-size={batch_size} '\n",
        "#                 f'> ~/benches/{model}/{task}/{input_size}_{batch_size}_{index:02d}.jl'\n",
        "#             )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%run -n main\n",
        "bench_registry = list(list_bench_registry())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## show"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%run -n main\n",
        "records = query_bench_registry(\n",
        "    bench_registry,\n",
        "    input_size=1,\n",
        "    batch_size=1,\n",
        ")\n",
        "\n",
        "records = query_bench_registry(\n",
        "    bench_registry,\n",
        "    input_size=2000,\n",
        "    batch_size=32,\n",
        "    task=(TERRA),\n",
        "    model=(RUBERT, RUGPT3_MEDIUM)\n",
        ")\n",
        "\n",
        "benches = [\n",
        "    load_bench_registry(_)\n",
        "    for _ in records\n",
        "]\n",
        "\n",
        "# task=(DANETQA, PARUS, RCB, RWSD, RUSSE, TERRA, LIDIRUS)\n",
        "# task=(RUCOS, MUSERC)\n",
        "\n",
        "# model=(RUBERT, RUBERT_CONVERSATIONAL, BERT_MULTILINGUAL)\n",
        "# model=(RUGPT3_LARGE, RUGPT3_MEDIUM, RUGPT3_SMALL)\n",
        "\n",
        "show_benches(benches)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## report"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%run -n main\n",
        "bench_groups = list(load_group_benches(bench_registry))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### gpu ram"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%run -n main\n",
        "show_gpu_ram_bench_report(bench_groups)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%run -n main\n",
        "table = show_gpu_ram_bench_report2(bench_groups)\n",
        "table"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%run -n main\n",
        "show_gpu_ram_hub_size_bench_report(bench_groups)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### time"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%run -n main\n",
        "show_init_time_bench_report(bench_groups)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%run -n main\n",
        "show_proc_time_bench_report(bench_groups)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%run -n main\n",
        "table = show_rps_bench_report(bench_groups)\n",
        "table"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# score perf"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# %run -n main\n",
        "# show_score_perf(docker_scores, leaderboard, bench_groups)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": false
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "nteract": {
      "version": "0.23.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}